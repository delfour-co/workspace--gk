# ai-runtime - AI Orchestration Engine

**Le cÅ“ur du systÃ¨me GK** - Orchestre un LLM local avec des MCP servers pour crÃ©er une interface conversationnelle.

## ğŸ¯ Vision

Permettre Ã  l'utilisateur d'interagir avec tous les services (mail, chat, calendrier) via une interface conversationnelle naturelle:

```
User: "Envoie un email Ã  john@example.com pour lui dire bonjour"
AI: "âœ… Email envoyÃ© Ã  john@example.com avec le sujet 'Bonjour'"

User: "Quels sont mes mails importants aujourd'hui?"
AI: "Tu as 3 emails importants:
     1. De alice@example.com: RÃ©union demain
     2. De bob@company.com: Contrat Ã  signer
     3. De team@startup.io: Release notes v2.0"
```

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           web-ui / CLI                   â”‚
â”‚        (WebSocket / HTTP)                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           ai-runtime                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚   Conversation Manager          â”‚   â”‚
â”‚  â”‚   - Context history             â”‚   â”‚
â”‚  â”‚   - Message queue               â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                 â”‚                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚      LLM Engine                 â”‚   â”‚
â”‚  â”‚   - Model loading               â”‚   â”‚
â”‚  â”‚   - Inference                   â”‚   â”‚
â”‚  â”‚   - Tool calling                â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                 â”‚                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚    MCP Orchestrator             â”‚   â”‚
â”‚  â”‚   - Server registry             â”‚   â”‚
â”‚  â”‚   - Tool discovery              â”‚   â”‚
â”‚  â”‚   - Request routing             â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚         â”‚         â”‚
        â–¼         â–¼         â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ mcp-   â”‚ â”‚ mcp-   â”‚ â”‚ mcp-   â”‚
   â”‚ mail   â”‚ â”‚ chat   â”‚ â”‚ cal    â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚         â”‚         â”‚
        â–¼         â–¼         â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚mail-rs â”‚ â”‚chat-rs â”‚ â”‚dav-rs  â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ Features

### Phase 1: MVP (Cette semaine)

- âœ… LLM local loading (mock pour tests)
- âœ… MCP protocol basique (JSON-RPC)
- âœ… MCP server registry
- âœ… Tool calling vers mcp-mail-server
- âœ… API HTTP simple (/chat endpoint)
- âœ… Test E2E: Envoyer email via conversation

### Phase 2: Production

- âšª LLM rÃ©el (Mistral 7B / Llama 3.1 8B)
- âšª Streaming responses (WebSocket)
- âšª Context window management
- âšª Multiple MCP servers
- âšª Tool chaining (multi-step tasks)
- âšª Error recovery
- âšª Rate limiting

## ğŸ“‹ MCP Protocol

Le **Model Context Protocol** (MCP) est un protocole standard pour connecter des LLMs Ã  des outils externes.

### Format Message

```json
{
  "jsonrpc": "2.0",
  "method": "tools/call",
  "params": {
    "name": "send_email",
    "arguments": {
      "to": "john@example.com",
      "subject": "Hello",
      "body": "Hi John!"
    }
  },
  "id": 1
}
```

### Tools Disponibles

#### mcp-mail-server

- `send_email(to, subject, body)` - Envoyer un email
- `list_emails(folder, limit)` - Lister les emails
- `read_email(id)` - Lire un email
- `search_emails(query)` - Rechercher emails

## ğŸ”§ Stack Technique

- **Rust** (async/await avec Tokio)
- **LLM Engine**: llama.cpp bindings ou candle
- **MCP**: JSON-RPC over HTTP/WebSocket
- **API**: Axum framework
- **Config**: TOML

## ğŸ“¦ Structure

```
ai-runtime/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ main.rs              # Entry point + HTTP server
â”‚   â”œâ”€â”€ llm/
â”‚   â”‚   â”œâ”€â”€ mod.rs           # LLM engine trait
â”‚   â”‚   â”œâ”€â”€ mock.rs          # Mock LLM for testing
â”‚   â”‚   â””â”€â”€ mistral.rs       # Real LLM (future)
â”‚   â”œâ”€â”€ mcp/
â”‚   â”‚   â”œâ”€â”€ mod.rs           # MCP protocol types
â”‚   â”‚   â”œâ”€â”€ server.rs        # MCP server client
â”‚   â”‚   â”œâ”€â”€ registry.rs      # Server registry
â”‚   â”‚   â””â”€â”€ tool.rs          # Tool definition
â”‚   â”œâ”€â”€ conversation/
â”‚   â”‚   â”œâ”€â”€ mod.rs           # Conversation manager
â”‚   â”‚   â”œâ”€â”€ context.rs       # Context window
â”‚   â”‚   â””â”€â”€ message.rs       # Message types
â”‚   â””â”€â”€ config.rs            # Configuration
â”œâ”€â”€ Cargo.toml
â””â”€â”€ config.toml              # Runtime config
```

## ğŸ¯ Quick Start

### 1. DÃ©marrer mail-rs

```bash
cd mail-rs
cargo run --release
```

### 2. DÃ©marrer mcp-mail-server

```bash
cd mcp-mail-server
cargo run
```

### 3. DÃ©marrer ai-runtime

```bash
cd ai-runtime
cargo run
```

### 4. Tester

```bash
curl -X POST http://localhost:8888/chat \
  -H "Content-Type: application/json" \
  -d '{
    "message": "Envoie un email Ã  john@example.com pour dire bonjour"
  }'
```

## ğŸ“ Configuration

```toml
[server]
listen_addr = "0.0.0.0:8888"

[llm]
model = "mock"  # or "mistral-7b", "llama-3.1-8b"
model_path = "./models/mistral-7b-instruct.gguf"
context_size = 4096

[mcp]
servers = [
  { name = "mail", url = "http://localhost:8090" }
]
```

## ğŸ” SÃ©curitÃ©

- âš ï¸ **Pas d'authentification** pour MVP
- Production: JWT tokens
- Rate limiting sur API
- Validation inputs LLM

## ğŸ§ª Tests

```bash
# Tests unitaires
cargo test

# Test avec mock LLM
cargo run --features mock

# Test E2E
./scripts/test-e2e.sh
```

## ğŸ“š Ressources

- [MCP Specification](https://spec.modelcontextprotocol.io/)
- [llama.cpp](https://github.com/ggerganov/llama.cpp)
- [Candle](https://github.com/huggingface/candle)
- [Mistral Models](https://mistral.ai/technology/#models)

## ğŸ¯ Milestone

**MVP validÃ© quand**: On peut envoyer un email via une commande en langage naturel.

**Success criteria**:
- âœ… API `/chat` rÃ©pond
- âœ… LLM parse l'intent "envoyer email"
- âœ… Tool calling vers mcp-mail-server
- âœ… Email effectivement envoyÃ© via mail-rs
- âœ… RÃ©ponse confirmative Ã  l'utilisateur
